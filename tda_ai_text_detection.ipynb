{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aguilin1/tda_ai_text_generation/blob/main/tda_ai_text_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aguilin1/tda_ai_text_generation.git\n",
        "!pip install nltk\n",
        "!pip install Ripser"
      ],
      "metadata": {
        "id": "rNoNDVYHKGF_",
        "outputId": "da6c566d-4656-435b-a1ab-11dad472940a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tda_ai_text_generation'...\n",
            "remote: Enumerating objects: 161, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 161 (delta 15), reused 0 (delta 0), pack-reused 123 (from 2)\u001b[K\n",
            "Receiving objects: 100% (161/161), 7.74 MiB | 4.47 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting Ripser\n",
            "  Downloading ripser-0.6.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.11/dist-packages (from Ripser) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from Ripser) (2.0.2)\n",
            "Collecting persim (from Ripser)\n",
            "  Downloading persim-0.3.8-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from Ripser) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from Ripser) (1.6.1)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.11/dist-packages (from persim->Ripser) (1.2.18)\n",
            "Collecting hopcroftkarp (from persim->Ripser)\n",
            "  Downloading hopcroftkarp-1.2.5.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from persim->Ripser) (1.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from persim->Ripser) (3.10.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->Ripser) (3.6.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated->persim->Ripser) (1.17.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->persim->Ripser) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->persim->Ripser) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->persim->Ripser) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->persim->Ripser) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->persim->Ripser) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->persim->Ripser) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->persim->Ripser) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->persim->Ripser) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->persim->Ripser) (1.17.0)\n",
            "Downloading ripser-0.6.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (841 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.3/841.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading persim-0.3.8-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.6/48.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: hopcroftkarp\n",
            "  Building wheel for hopcroftkarp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hopcroftkarp: filename=hopcroftkarp-1.2.5-py2.py3-none-any.whl size=18104 sha256=acf8a3b97a7e76aa7e43186111d3c2b89960b9cd9a72f4da078b1c6289d0bea2\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/cc/2d/de23a8b9ae586817b0b44de4a4b1a08f23473e248a644b312f\n",
            "Successfully built hopcroftkarp\n",
            "Installing collected packages: hopcroftkarp, persim, Ripser\n",
            "Successfully installed Ripser-0.6.12 hopcroftkarp-1.2.5 persim-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set up Ripser for use (run this just once)\n",
        "\n",
        "from ripser import ripser, Rips\n",
        "from persim import plot_diagrams"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RqDf00naAgYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions ---\n",
        "# cosSimDistanceMatrix: creates cosine similarity distance matrix\n",
        "# vrFiltration: takes a distance matrix and computes Vietoris-Rips filtration\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import itertools\n",
        "import warnings\n",
        "from scipy.sparse import SparseEfficiencyWarning\n",
        "warnings.simplefilter('ignore',SparseEfficiencyWarning)\n",
        "\n",
        "def smoothing(embeddings):\n",
        "  number_sentences = embeddings.shape[0]\n",
        "  original_embeddings = embeddings.copy()\n",
        "  for i in range(number_sentences):\n",
        "    smoothed = original_embeddings[safe_index(i -3, number_sentences)]/8.0 +\\\n",
        "      original_embeddings[safe_index(i -2, number_sentences)]/4.0 + \\\n",
        "      original_embeddings[safe_index(i -1, number_sentences)]/2.0 + \\\n",
        "      original_embeddings[i] + \\\n",
        "      original_embeddings[safe_index(i +1, number_sentences)]/2.0 + \\\n",
        "      original_embeddings[safe_index(i +2, number_sentences)]/4.0 + \\\n",
        "      original_embeddings[safe_index(i +3, number_sentences)]/8.0\n",
        "    embeddings[i] = smoothed\n",
        "  return embeddings\n",
        "\n",
        "def safe_index(i, total_n):\n",
        "  if i < 0:\n",
        "    return i * -1\n",
        "  elif i >= total_n-1:\n",
        "    return total_n -1 - (i % total_n)\n",
        "  else:\n",
        "    return i\n",
        "\n",
        "\n",
        "def angular_distance(embeddings, index_1, index_2):\n",
        "  cos_sim = cosine_similarity(embeddings[index_1].reshape(1, -1),\\\n",
        "                              embeddings[index_2].reshape(1, -1))[0][0]\n",
        "\n",
        "  dist = 2 * np.arccos(cos_sim) / np.pi\n",
        "  return dist\n",
        "\n",
        "# input a single array of embeddings\n",
        "def angular_distance_matrix(embeddings, time_skeleton = False):\n",
        "  # get # of data points in each sample\n",
        "  n_dpts = embeddings.shape[0]\n",
        "  # preallocate distance matrix\n",
        "  angular_distances = np.zeros((n_dpts, n_dpts))\n",
        "  # Label each data point in order starting at 0, 1, 2, ...\n",
        "  # Distance between data point i and data point j is in (i,j) entry of matrix\n",
        "  # Matrix will be upper triangular\n",
        "  for pair1_i, pair2_i in itertools.combinations(range(n_dpts), 2):\n",
        "    if time_skeleton and (pair1_i == pair2_i + 1 or pair1_i == pair2_i - 1):\n",
        "      dist = 0\n",
        "    else:\n",
        "      dist = angular_distance(embeddings, pair1_i, pair2_i)\n",
        "    angular_distances[pair1_i][pair2_i] = dist\n",
        "    angular_distances[pair2_i][pair1_i] = dist\n",
        "  return angular_distances\n",
        "\n",
        "\n",
        "def mixed_distance_matrix(bow_embeddings, bert_embeddings, alpha, time_skeleton = False):\n",
        "  # This should align with the tokens (number of sentences), so match between types\n",
        "  n_dpts = bert_embeddings.shape[0]\n",
        "\n",
        "  cosSimDistances = np.zeros((n_dpts, n_dpts))\n",
        "\n",
        "  for pair1_i, pair2_i in itertools.combinations(range(n_dpts), 2):\n",
        "    if time_skeleton and (pair1_i == pair2_i + 1 or pair1_i == pair2_i - 1):\n",
        "      dist = 0\n",
        "    else:\n",
        "      distance_bow = angular_distance(bow_embeddings, pair1_i, pair2_i)\n",
        "      distance_bert = angular_distance(bert_embeddings, pair1_i, pair2_i)\n",
        "      dist = alpha * distance_bow + (1 - alpha) * distance_bert\n",
        "\n",
        "    cosSimDistances[pair1_i][pair2_i] = dist\n",
        "    cosSimDistances[pair2_i][pair1_i] = dist\n",
        "  return cosSimDistances\n",
        "\n",
        "# input a symmetric distance matrix and this function will return birth and\n",
        "# death homology data for the V-R complex\n",
        "def ripserFiltration(distanceMat, maxDim=2):\n",
        "  # Here's one way to use Ripser\n",
        "  # examples and instructions are here:\n",
        "  # https://ripser.scikit-tda.org/en/latest/notebooks/Basic%20Usage.html\n",
        "  # https://docs.scikit-tda.org/en/latest/notebooks/scikit-tda%20Tutorial.html#1.1.3.-Input-option:-Distance-matrix\n",
        "\n",
        "  # This version uses the distance matrix input, which is what we want\n",
        "  birthDeathPairs = ripser(distanceMat, distance_matrix=True, maxdim=maxDim)['dgms']\n",
        "\n",
        "  # diagrams is a vector of vectors where each element of each\n",
        "  # vector is [birth epoch, death epoch] and the vectors are in increasing order\n",
        "  # of homology (H0, H1, H2, etc.).\n",
        "  return birthDeathPairs"
      ],
      "metadata": {
        "id": "_mJJSjn9kAmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from ripser import Rips\n",
        "from persim import PersImage\n",
        "from persim import PersistenceImager\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def diagram_sizes(dgms):\n",
        "    return {f\"H_{i}\": len(d) for i, d in enumerate(dgms)}\n",
        "\n",
        "def load_cached_embeddings(abstract_type, embedding_type):\n",
        "  DATA_FILE_BASE = '/content/tda_ai_text_generation/data/{}_encodings_{}-{}.pkl'\n",
        "\n",
        "  bert_data = DATA_FILE_BASE.format('sentence-bert', abstract_type, 1)\n",
        "  bow_data = DATA_FILE_BASE.format('bow', abstract_type, 0)\n",
        "\n",
        "\n",
        "  if embedding_type == 'bert':\n",
        "    with open(bert_data, \"rb\") as data_file:\n",
        "        cache_data = pickle.load(data_file)\n",
        "        batch_sentences = cache_data['sentences']\n",
        "        batch_embeddings = cache_data['embeddings']\n",
        "  elif embedding_type == 'bow':\n",
        "    with open(bow_data, \"rb\") as data_file:\n",
        "        cache_data = pickle.load(data_file)\n",
        "        batch_sentences = cache_data['sentences']\n",
        "        batch_embeddings = cache_data['embeddings']\n",
        "  return batch_sentences, batch_embeddings\n",
        "\n",
        "def print_sizes(sizes, num_abstracts):\n",
        "  num_h0 = 0\n",
        "  num_h1 = 0\n",
        "  num_h2 = 0\n",
        "  for size in sizes:\n",
        "    num_h0 += size['H_0']\n",
        "    num_h1 += size['H_1']\n",
        "    num_h2 += size['H_2']\n",
        "\n",
        "  print(\"|H0|: {} |H1|: {} |H2|: {}\".format(num_h0/num_abstracts, num_h1/num_abstracts, num_h2/num_abstracts))\n",
        "\n",
        "# replace with should be multiplied by the max, right? 1.1 could be anywhere in the diagram?\n",
        "def replace_infinity(diagrams, replace_with = 1.1):\n",
        "  for diagram in diagrams:\n",
        "    for point in diagram:\n",
        "      if point[1] == np.inf:\n",
        "        point[1] = replace_with\n",
        "  return diagrams\n",
        "\n",
        "# Eli: it might be better to just remove inf points; I think that Persistence Images were designed that way based on the paper.\n",
        "def remove_infinity(diagrams):\n",
        "  for diagram in diagrams:\n",
        "    for point in diagram:\n",
        "      if point[1] == np.inf:\n",
        "        point = []\n",
        "  return diagrams\n",
        "\n",
        "def run_evaluation(abstract_type, embedding_type, time_skeleton = False, with_smoothing = False, graph = False, num_abstracts = 6):\n",
        "  _, batch_embeddings = load_cached_embeddings(abstract_type, embedding_type)\n",
        "\n",
        "  # initialize diagram arrays\n",
        "  diagrams_h0 = []\n",
        "  diagrams_h1 = []\n",
        "  diagrams_h2 = []\n",
        "\n",
        "  rips = Rips()\n",
        "  persistence_diagrams = []\n",
        "  sizes = []\n",
        "  description = \"{} text encoded with {}\".format(abstract_type, embedding_type.upper())\n",
        "  if time_skeleton:\n",
        "    description += \" with time skeleton\"\n",
        "  if with_smoothing:\n",
        "    description += \" smoothed\"\n",
        "  if graph:\n",
        "    num_abstracts = 3\n",
        "    fig,axs = plt.subplots(nrows=2,ncols=3,figsize=(12,6))\n",
        "    title = \"Persistence diagram for {}\".format(description)\n",
        "    fig.suptitle(title)\n",
        "    subplot = 0\n",
        "  else:\n",
        "    print(description)\n",
        "  for i, embeddings in enumerate(batch_embeddings[0:num_abstracts]):\n",
        "    if with_smoothing:\n",
        "      embeddings = smoothing(embeddings)\n",
        "    distances = angular_distance_matrix(embeddings, time_skeleton)\n",
        "    diagram = ripserFiltration(distances)\n",
        "    persistence_diagrams.append(diagram)\n",
        "    sizes.append(diagram_sizes(diagram))\n",
        "\n",
        "    # instantiate PeristanceImager to use in plotting\n",
        "    pimgr = PersistenceImager(pixel_size=0.05)\n",
        "    # pimgr.fit(replace_infinity(diagram))\n",
        "\n",
        "    if graph:\n",
        "      imgs = pimgr.transform(replace_infinity(diagram))\n",
        "      pimgr.plot_image(imgs[0], axs[1][subplot])\n",
        "      rips.plot(diagram, show=False, ax=axs[0][subplot])\n",
        "      subplot += 1\n",
        "\n",
        "    # For each new diagram, append its points to the the appropriate array. These must have all infs removed or replaced.\n",
        "    diagram_infResolved = replace_infinity(diagram)\n",
        "    # skip any empty ones\n",
        "    if len(diagram_infResolved[0]) > 0:\n",
        "      diagrams_h0.append(diagram_infResolved[0])\n",
        "    if len(diagram_infResolved[1]) > 0:\n",
        "      diagrams_h1.append(diagram_infResolved[1])\n",
        "    if len(diagram_infResolved[2]) > 0:\n",
        "      diagrams_h2.append(diagram_infResolved[2])\n",
        "\n",
        "  # create labels array for regression analysis later--all zeros for ai, all ones for human\n",
        "  if abstract_type == 'ai':\n",
        "    labelsH0 = np.zeros(len(diagrams_h0))\n",
        "    labelsH1 = np.zeros(len(diagrams_h1))\n",
        "    labelsH2 = np.zeros(len(diagrams_h2))\n",
        "  else:\n",
        "    labelsH0 = np.ones(len(diagrams_h0))\n",
        "    labelsH1 = np.ones(len(diagrams_h1))\n",
        "    labelsH2 = np.ones(len(diagrams_h2))\n",
        "\n",
        "  if graph:\n",
        "    plt.show()\n",
        "\n",
        "  print_sizes(sizes, num_abstracts)\n",
        "\n",
        "  return persistence_diagrams, diagrams_h0, diagrams_h1, diagrams_h2, labelsH0, labelsH1, labelsH2\n",
        "\n",
        "\n",
        "def run_mixed_encoding_evaluation(abstract_type, alpha, num_abstracts=6, time_skeleton = False, plot=False):\n",
        "  _, batch_bow_embeddings = load_cached_embeddings(abstract_type, 'bow')\n",
        "  _, batch_bert_embeddings = load_cached_embeddings(abstract_type, 'bert')\n",
        "  if alpha < 0.0 and alpha > 1.0:\n",
        "    raise ValueError(\"alpha must be between 0 and 1\")\n",
        "\n",
        "  rips = Rips()\n",
        "  persistence_diagrams = []\n",
        "  sizes = []\n",
        "  description = \"{} mixed encoding with alpha={}\".format(abstract_type, alpha)\n",
        "  if time_skeleton:\n",
        "    description += \" with time skeleton\"\n",
        "  if plot:\n",
        "    fig,axs = plt.subplots(nrows=2,ncols=3,figsize=(12,6))\n",
        "    title = \"Persistence diagrams for {}\".format(description)\n",
        "    fig.suptitle(title)\n",
        "    subplot = 0\n",
        "  else:\n",
        "    print(description)\n",
        "  for i in range(num_abstracts):\n",
        "    pimgr = PersistenceImager(pixel_size=0.05)\n",
        "    distances = mixed_distance_matrix(batch_bow_embeddings[i], batch_bert_embeddings[i], alpha, time_skeleton)\n",
        "    diagram = ripserFiltration(distances)\n",
        "    persistence_diagrams.append(diagram)\n",
        "    sizes.append(diagram_sizes(diagram))\n",
        "\n",
        "    if plot:\n",
        "      imgs = pimgr.transform(replace_infinity(diagram))\n",
        "      pimgr.plot_image(imgs[0], axs[1][subplot])\n",
        "      rips.plot(diagram, show=False, ax=axs[0][subplot])\n",
        "      subplot += 1\n",
        "\n",
        "  if plot:\n",
        "    plt.show()\n",
        "\n",
        "  print_sizes(sizes, num_abstracts)\n",
        "  return persistence_diagrams\n",
        "\n",
        "# initialize arrays to hold data for both the ai and human cases\n",
        "labelsH0_actual = []\n",
        "labelsH1_actual = []\n",
        "labelsH2_actual = []\n",
        "diagrams_h0_actual = []\n",
        "diagrams_h1_actual = []\n",
        "diagrams_h2_actual = []\n",
        "\n",
        "# run ai evaluations\n",
        "_, dH0, dH1, dH2, lblH0, lblH1, lblH2 = run_evaluation('ai', 'bow', graph=False)\n",
        "# run_evaluation('ai', 'bow', with_smoothing=True)\n",
        "##run_evaluation('ai', 'bow', time_skeleton=True, graph=True)\n",
        "\n",
        "# add ai run data to holding arrays\n",
        "labelsH0_actual.extend(lblH0)\n",
        "labelsH1_actual.extend(lblH1)\n",
        "labelsH2_actual.extend(lblH2)\n",
        "diagrams_h0_actual.extend(dH0)\n",
        "diagrams_h1_actual.extend(dH1)\n",
        "diagrams_h2_actual.extend(dH2)\n",
        "\n",
        "# run human evaluations\n",
        "_, dH0, dH1, dH2, lblH0, lblH1, lblH2 = run_evaluation('human', 'bow', graph=False)\n",
        "# run_evaluation('human', 'bow', with_smoothing=True)\n",
        "##run_evaluation('human', 'bow')\n",
        "# run_evaluation('human', 'bow', with_smoothing=True)\n",
        "##run_evaluation('human', 'bow', time_skeleton=True)\n",
        "\n",
        "# add human run data to holding arrays\n",
        "labelsH0_actual.extend(lblH0)\n",
        "labelsH1_actual.extend(lblH1)\n",
        "labelsH2_actual.extend(lblH2)\n",
        "diagrams_h0_actual.extend(dH0)\n",
        "diagrams_h1_actual.extend(dH1)\n",
        "diagrams_h2_actual.extend(dH2)\n",
        "\n",
        "# other evaluations\n",
        "##run_evaluation('ai', 'bert')\n",
        "# run_evaluation('ai', 'bert', with_smoothing=True)\n",
        "##run_evaluation('human', 'bert')\n",
        "\n",
        "##run_mixed_encoding_evaluation('ai', 0.5, 3, time_skeleton=True, plot=True)\n",
        "##run_mixed_encoding_evaluation('ai', 0.5, 3);\n",
        "# run_mixed_encoding_evaluation('human', 0.5, 3, time_skeleton=True)\n",
        "# run_mixed_encoding_evaluation('human', 0.5, 3)\n",
        "\n",
        "\n",
        "# transform peristence diagrams into persistence images and generate flat vector versions of the PIs for user in regression\n",
        "# for H0, all the birth values are 0. This is specially handled by duplicating all the pairs and setting the duplicate birth values to 1 so that there is a 2-dimensionality in the resulting diagram\n",
        "if len(diagrams_h0_actual) > 0:\n",
        "  # extend H0 to avoid error when all births are zero\n",
        "  d = copy.deepcopy(diagrams_h0_actual)\n",
        "  l = copy.deepcopy(labelsH0_actual)\n",
        "  for diag in d:\n",
        "    diag[:,0]=1\n",
        "  diagrams_h0_actual.extend(d)\n",
        "  labelsH0_actual.extend(l)\n",
        "  # instantiate PersistenceImager, transform PD to PI, and flatten everything for regression analysis\n",
        "  pimgrH0 = PersistenceImager(pixel_size=0.01)\n",
        "  pimgrH0.fit(diagrams_h0_actual)\n",
        "  imgsH0 = pimgrH0.transform(diagrams_h0_actual, skew=False)\n",
        "  imgsH0_array = np.array([imgH0.flatten() for imgH0 in imgsH0])\n",
        "# for H1\n",
        "if len(diagrams_h1_actual) > 0:\n",
        "  # instantiate PersistenceImager, transform PD to PI, and flatten everything for regression analysis\n",
        "  pimgrH1 = PersistenceImager(pixel_size=0.01)\n",
        "  pimgrH1.fit(diagrams_h1_actual)\n",
        "  imgsH1 = pimgrH1.transform(diagrams_h1_actual, skew=True)\n",
        "  imgsH1_array = np.array([imgH1.flatten() for imgH1 in imgsH1])\n",
        "# for H2\n",
        "if len(diagrams_h2_actual) > 0:\n",
        "  # instantiate PersistenceImager, transform PD to PI, and flatten everything for regression analysis\n",
        "  pimgrH2 = PersistenceImager(pixel_size=0.01)\n",
        "  pimgrH2.fit(diagrams_h2_actual)\n",
        "  imgsH2 = pimgrH2.transform(diagrams_h2_actual, skew=True)\n",
        "  imgsH2_array = np.array([imgH2.flatten() for imgH2 in imgsH2])\n",
        "\n",
        "# Perform regression analysis on all dimensions\n",
        "if len(diagrams_h0_actual) > 0:\n",
        "  X0_train, X0_test, y0_train, y0_test = train_test_split(imgsH0_array, labelsH0_actual, train_size=0.80, random_state=42)\n",
        "  lrH0 = LogisticRegression()\n",
        "  lrH0.fit(X0_train, y0_train)\n",
        "  scr0 = lrH0.score(X0_test, y0_test)\n",
        "  pred0 = lrH0.predict(X0_test)\n",
        "  print('H0: predicted classification(s): ', pred0)\n",
        "  print('H0: actual classification(s): ', y0_test)\n",
        "  print('H0: classification success rate was', scr0)\n",
        "\n",
        "if len(diagrams_h1_actual) > 0:\n",
        "  X1_train, X1_test, y1_train, y1_test = train_test_split(imgsH1_array, labelsH1_actual, train_size=0.80, random_state=42)\n",
        "  lrH1 = LogisticRegression()\n",
        "  lrH1.fit(X1_train, y1_train)\n",
        "  scr1 = lrH1.score(X1_test, y1_test)\n",
        "  pred1 = lrH1.predict(X1_test)\n",
        "  print('H1: predicted classification: ', pred1)\n",
        "  print('H1: actual classification: ', y1_test)\n",
        "  print('H1: classification success rate was', scr1)\n",
        "\n",
        "if len(diagrams_h2_actual) > 0:\n",
        "  X2_train, X2_test, y2_train, y2_test = train_test_split(imgsH2_array, labelsH2_actual, train_size=0.80, random_state=42)\n",
        "  lrH2 = LogisticRegression()\n",
        "  lrH2.fit(X2_train, y2_train)\n",
        "  scr2 = lrH2.score(X2_test, y2_test)\n",
        "  pred2 = lrH2.predict(X2_test)\n",
        "  print('H2: predicted classification: ', pred2)\n",
        "  print('H2: actual classification: ', y2_test)\n",
        "  print('H2: classification success rate was', scr2)"
      ],
      "metadata": {
        "id": "UuzdYItZKcti",
        "outputId": "c9b6a0bc-4a2d-4efc-c161-17cb5abf4c45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rips(maxdim=1, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
            "ai text encoded with BOW\n",
            "|H0|: 7.666666666666667 |H1|: 0.5 |H2|: 0.0\n",
            "Rips(maxdim=1, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
            "human text encoded with BOW\n",
            "|H0|: 8.166666666666666 |H1|: 0.5 |H2|: 0.0\n",
            "H0: predicted classification(s):  [0. 0. 0. 0. 0.]\n",
            "H0: actual classification(s):  [np.float64(1.0), np.float64(0.0), np.float64(0.0), np.float64(1.0), np.float64(1.0)]\n",
            "H0: classification success rate was 0.4\n",
            "H1: predicted classification:  [1.]\n",
            "H1: actual classification:  [np.float64(0.0)]\n",
            "H1: classification success rate was 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## To wrap the array outputs for readability\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "12Fn48Vw6858"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather 3 sets: human cases, AI cases, and test (\"unknown\") cases\n",
        "\n",
        "# Determine a representative persistence diagram for human cases and AI cases? HOW?\n",
        "\n",
        "# Compare each of the test cases against the representative diagrams, classify,\n",
        "# and determine whether classification was successful or not\n",
        "\n",
        "# suggest using\n",
        "# https://persim.scikit-tda.org/en/latest/notebooks/distances.html\n",
        "\n",
        "# Example\n",
        "# if A and B are two Ripser persistence diagrams to be compared\n",
        "#distance = persim.bottleneck(A, B, matching=False)\n",
        "# and then just see whether the test case distance from human diagram is smaller\n",
        "# or larger than distance from AI diagram. Classify based on smaller distance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "cWKjUzh_KPyb",
        "outputId": "33008398-02a2-49b2-dc6e-e4d4c8df4834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}